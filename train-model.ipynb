{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79edb39",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14860,
     "status": "ok",
     "timestamp": 1718277208088,
     "user": {
      "displayName": "İbrahim",
      "userId": "17409863343183496223"
     },
     "user_tz": -120
    },
    "id": "e79edb39",
    "outputId": "0e251b1c-50d0-490a-c632-30cdb95a7d13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of train:  (58317, 55)\n",
      "# of test:  (73729, 55)\n",
      "# of labels:  (73729,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from utils import read_data, iterate_batches, apply_adjustment, sliding_window_anomaly_detection, get_precision_recall_f1\n",
    "\n",
    "window_size = 50  # size of the window\n",
    "dataset_list = [\"MSL\", \"PSM\", \"SMAP\", \"SMD\", \"SWAT\"]\n",
    "dataset = dataset_list[2]\n",
    "\n",
    "# **** First train AE or VAE\n",
    "flag_train_composite = True\n",
    "flag_AE = False # True for AE, False for VAE\n",
    "\n",
    "# # **** Then you are ready for AE-FAR or VAE-FAR!\n",
    "# flag_train_composite = True\n",
    "# flag_AE = True\n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "if flag_train_composite:\n",
    "    if flag_AE:\n",
    "        MODEL_PATH = \"models/\" + dataset + \"/\" + dataset + \"-AE-FAR\"\n",
    "    else:\n",
    "        MODEL_PATH = \"models/\" + dataset + \"/\" + dataset + \"-VAE-FAR\"\n",
    "else:\n",
    "    if flag_AE:\n",
    "        MODEL_PATH = \"models/\" + dataset + \"/\" + dataset + \"-AE\"\n",
    "    else:\n",
    "        MODEL_PATH = \"models/\" + dataset + \"/\" + dataset + \"-VAE\"\n",
    "\n",
    "\n",
    "train_data, test_data, val_data, test_labels = read_data(dataset)\n",
    "input_dim = train_data.shape[1]\n",
    "\n",
    "print(\"# of train: \", train_data.shape)\n",
    "print(\"# of test: \", test_data.shape)\n",
    "print(\"# of labels: \", test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6649e841",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 374,
     "status": "ok",
     "timestamp": 1718277013990,
     "user": {
      "displayName": "İbrahim",
      "userId": "17409863343183496223"
     },
     "user_tz": -120
    },
    "id": "6649e841",
    "outputId": "731104a0-a764-4502-babb-fbf777d27eea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of 1s in test:  7766\n",
      "number of 0s in test:  65963\n"
     ]
    }
   ],
   "source": [
    "df_test_0 = test_labels[test_labels == 0]\n",
    "df_test_1 = test_labels[test_labels == 1]\n",
    "\n",
    "print(\"number of 1s in test: \", len(df_test_1))\n",
    "print(\"number of 0s in test: \", len(df_test_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e7e78c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "executionInfo": {
     "elapsed": 1282225,
     "status": "error",
     "timestamp": 1718284795051,
     "user": {
      "displayName": "İbrahim",
      "userId": "17409863343183496223"
     },
     "user_tz": -120
    },
    "id": "a6e7e78c",
    "outputId": "2b2ae169-5c46-49da-d774-fd0649e232db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([58317, 55])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_290528/1092730607.py:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  autoencoder = torch.load(\"models/MSL/MSL-model.pt\", map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - 1.2466344833374023 - train loss: 0.5425175858340879  val loss: 0.49428130594686004\n",
      "1 - 1.2491939067840576 - train loss: 0.5356970922166829  val loss: 0.49044155508886184\n",
      "2 - 1.2135224342346191 - train loss: 0.5307539100073898  val loss: 0.4839992885475282\n",
      "3 - 1.1617608070373535 - train loss: 0.5264139387431338  val loss: 0.4795556391114727\n",
      "4 - 1.1401846408843994 - train loss: 0.5239369118518656  val loss: 0.4768094210021945\n",
      "5 - 1.1518685817718506 - train loss: 0.521661089011902  val loss: 0.4734702498324543\n",
      "6 - 1.1960818767547607 - train loss: 0.5182752672813525  val loss: 0.46840256189154766\n",
      "7 - 1.184812068939209 - train loss: 0.5144706927621564  val loss: 0.46446636297284616\n",
      "8 - 1.1750593185424805 - train loss: 0.5119602879305858  val loss: 0.4618076448134341\n",
      "9 - 1.1142182350158691 - train loss: 0.5103117869289954  val loss: 0.4597504752966995\n",
      "10 - 1.082627534866333 - train loss: 0.5090342382932329  val loss: 0.4578418268254129\n",
      "11 - 1.177184820175171 - train loss: 0.5079460163862932  val loss: 0.4561412944739869\n",
      "12 - 1.1654202938079834 - train loss: 0.5069569853575351  val loss: 0.45460525819914066\n",
      "13 - 1.1339976787567139 - train loss: 0.5059985078059662  val loss: 0.45315124274354307\n",
      "14 - 1.1238644123077393 - train loss: 0.5049946256601783  val loss: 0.45165726644048404\n",
      "15 - 1.1082391738891602 - train loss: 0.5038140117864889  val loss: 0.44992360867084014\n",
      "16 - 1.074021339416504 - train loss: 0.5021799640643733  val loss: 0.44750788765268623\n",
      "17 - 1.1010172367095947 - train loss: 0.49952300060428106  val loss: 0.44362404064408373\n",
      "18 - 1.1253714561462402 - train loss: 0.49592396251835835  val loss: 0.43934410286217845\n",
      "19 - 1.1863114833831787 - train loss: 0.4928829807153338  val loss: 0.43616212749019634\n",
      "20 - 1.139249324798584 - train loss: 0.4906694778137694  val loss: 0.4336312134893468\n",
      "21 - 1.114469051361084 - train loss: 0.4888274187168894  val loss: 0.43131971757623605\n",
      "22 - 1.121872901916504 - train loss: 0.4870967702564054  val loss: 0.4290200935083104\n",
      "23 - 1.0801596641540527 - train loss: 0.48534734925902867  val loss: 0.4268240241671575\n",
      "24 - 1.1320843696594238 - train loss: 0.4836261271432848  val loss: 0.42471621363874074\n",
      "25 - 1.065645456314087 - train loss: 0.48194230459327964  val loss: 0.42258077411615586\n",
      "26 - 1.055614709854126 - train loss: 0.4802812111456501  val loss: 0.4204824160155073\n",
      "27 - 1.053053855895996 - train loss: 0.47869711938479315  val loss: 0.4183683082522763\n",
      "28 - 1.0723693370819092 - train loss: 0.47719557683095654  val loss: 0.41626010083204523\n",
      "29 - 1.0558342933654785 - train loss: 0.47577415211606167  val loss: 0.4142127676667379\n",
      "30 - 1.0436625480651855 - train loss: 0.47442082835703714  val loss: 0.4122483517698289\n",
      "31 - 1.051339864730835 - train loss: 0.4731115165053333  val loss: 0.4103375403543136\n",
      "32 - 1.0917866230010986 - train loss: 0.4718192502784261  val loss: 0.40844705886627125\n",
      "33 - 1.1083412170410156 - train loss: 0.4705244557998524  val loss: 0.4065480743775369\n",
      "34 - 1.0857181549072266 - train loss: 0.4692145663215534  val loss: 0.40461436066019174\n",
      "35 - 1.112248182296753 - train loss: 0.4678829932469709  val loss: 0.40263492995514893\n",
      "36 - 1.1769132614135742 - train loss: 0.4665289642130997  val loss: 0.4006169662237741\n",
      "37 - 1.1326677799224854 - train loss: 0.465155529708889  val loss: 0.398575656939871\n",
      "38 - 1.048285722732544 - train loss: 0.46376796228612777  val loss: 0.39653009141713463\n",
      "39 - 1.1034636497497559 - train loss: 0.462374158041616  val loss: 0.39450083142351156\n",
      "40 - 1.0583572387695312 - train loss: 0.46098363775544976  val loss: 0.3925039849249175\n",
      "41 - 1.1131339073181152 - train loss: 0.45960552304779667  val loss: 0.3905510857655247\n",
      "42 - 1.1864187717437744 - train loss: 0.45824762561830557  val loss: 0.3886530057624351\n",
      "43 - 1.1157197952270508 - train loss: 0.4569169183270245  val loss: 0.3868218869160206\n",
      "44 - 1.1998448371887207 - train loss: 0.455620086461925  val loss: 0.3850675524406872\n",
      "45 - 1.0524318218231201 - train loss: 0.45436233969188544  val loss: 0.3833911646909725\n",
      "46 - 1.0571613311767578 - train loss: 0.4531450558242157  val loss: 0.38178367108608097\n",
      "47 - 1.1125473976135254 - train loss: 0.4519649802654417  val loss: 0.38022907247132326\n",
      "48 - 1.1133439540863037 - train loss: 0.45081606279059644  val loss: 0.3787087575445504\n",
      "49 - 1.114457368850708 - train loss: 0.44969156943268007  val loss: 0.377207552110981\n",
      "50 - 1.0577139854431152 - train loss: 0.44858570833933126  val loss: 0.37571743063137425\n",
      "51 - 1.0982768535614014 - train loss: 0.44749439074197045  val loss: 0.3742372992264291\n",
      "52 - 1.1194989681243896 - train loss: 0.44641607496525565  val loss: 0.3727715833695953\n",
      "53 - 1.09263014793396 - train loss: 0.4453521157936114  val loss: 0.37132775756929604\n",
      "54 - 1.1935651302337646 - train loss: 0.44430608818051076  val loss: 0.3699120878328948\n",
      "55 - 1.1438300609588623 - train loss: 0.4432819343199841  val loss: 0.36852586385793984\n",
      "56 - 1.0717229843139648 - train loss: 0.4422818823798145  val loss: 0.36716528447692864\n",
      "57 - 1.0890636444091797 - train loss: 0.441305894850366  val loss: 0.36582447057794576\n",
      "58 - 1.1922123432159424 - train loss: 0.4403519874949939  val loss: 0.36449740282362225\n",
      "59 - 1.196242332458496 - train loss: 0.4394160802117416  val loss: 0.3631775823375722\n",
      "60 - 1.181236743927002 - train loss: 0.43849148037320684  val loss: 0.3618565846137334\n",
      "61 - 1.1818180084228516 - train loss: 0.4375692255584829  val loss: 0.3605229126233198\n",
      "62 - 1.1482875347137451 - train loss: 0.4366391734189452  val loss: 0.3591631914074482\n",
      "63 - 1.0636076927185059 - train loss: 0.4356917190010503  val loss: 0.3577639920979861\n",
      "64 - 1.0036625862121582 - train loss: 0.4347207145764147  val loss: 0.3563066172505637\n",
      "65 - 1.019477128982544 - train loss: 0.43372533658284257  val loss: 0.3547723322497824\n",
      "66 - 1.0037150382995605 - train loss: 0.43270598837848967  val loss: 0.35315601277973624\n",
      "67 - 1.0037012100219727 - train loss: 0.431663478666387  val loss: 0.3514708233511096\n",
      "68 - 1.0286476612091064 - train loss: 0.43060034040861767  val loss: 0.3497285725328826\n",
      "69 - 0.9973480701446533 - train loss: 0.4295152227986316  val loss: 0.3479376112771026\n",
      "70 - 1.002328872680664 - train loss: 0.42841102830667904  val loss: 0.3461249891494575\n",
      "71 - 1.0085740089416504 - train loss: 0.42730005946041755  val loss: 0.3443080194391679\n",
      "72 - 1.0151195526123047 - train loss: 0.42618863255496797  val loss: 0.3425063121383293\n",
      "73 - 1.0997846126556396 - train loss: 0.4250852390008763  val loss: 0.34073923938078704\n",
      "74 - 1.088165044784546 - train loss: 0.42399914304877984  val loss: 0.3390201551655492\n",
      "75 - 1.0976545810699463 - train loss: 0.4229378357608527  val loss: 0.33735930584473434\n",
      "76 - 1.0825016498565674 - train loss: 0.4219082326372269  val loss: 0.3357634683840673\n",
      "77 - 1.0845394134521484 - train loss: 0.42091696535518214  val loss: 0.33423900865223055\n",
      "78 - 1.1235294342041016 - train loss: 0.41996927151236024  val loss: 0.332788584625942\n",
      "79 - 1.083420991897583 - train loss: 0.4190688519420132  val loss: 0.3314144538848528\n",
      "80 - 0.9993894100189209 - train loss: 0.41821988205664273  val loss: 0.3301291211820361\n",
      "81 - 1.2261183261871338 - train loss: 0.41742580224675757  val loss: 0.32893909851170994\n",
      "82 - 1.0639455318450928 - train loss: 0.41668982249671227  val loss: 0.3278490677015385\n",
      "83 - 1.117480754852295 - train loss: 0.4160145454704769  val loss: 0.3268624740002861\n",
      "84 - 1.0043845176696777 - train loss: 0.41539746115737025  val loss: 0.3259703837243001\n",
      "85 - 1.1087653636932373 - train loss: 0.4148314416855883  val loss: 0.32515742364441874\n",
      "86 - 1.097771167755127 - train loss: 0.414310219854949  val loss: 0.3244069209501434\n",
      "87 - 1.078148603439331 - train loss: 0.41382904698423917  val loss: 0.3236966219162671\n",
      "88 - 1.0028116703033447 - train loss: 0.4133780763550979  val loss: 0.3230301606914049\n",
      "89 - 0.9999046325683594 - train loss: 0.4129640786466776  val loss: 0.3224925938789006\n",
      "90 - 0.9955027103424072 - train loss: 0.4125715344311047  val loss: 0.32203395369687826\n",
      "91 - 0.9916093349456787 - train loss: 0.4121993310785783  val loss: 0.32165392467004994\n",
      "92 - 0.9955191612243652 - train loss: 0.4118450219993017  val loss: 0.32110143011527975\n",
      "93 - 1.0557279586791992 - train loss: 0.41150827875594187  val loss: 0.32051639062874426\n",
      "94 - 1.0720853805541992 - train loss: 0.4111877031139381  val loss: 0.32000471370622185\n",
      "95 - 1.2689237594604492 - train loss: 0.41088423141381847  val loss: 0.3195275261558101\n",
      "96 - 1.0387303829193115 - train loss: 0.41059404662359666  val loss: 0.31908882317737075\n",
      "97 - 1.2332398891448975 - train loss: 0.41031532575451773  val loss: 0.3186797393757113\n",
      "98 - 1.1303091049194336 - train loss: 0.41004496217608805  val loss: 0.31827868894510136\n",
      "99 - 1.2026715278625488 - train loss: 0.4097818648313382  val loss: 0.31790382376421694\n",
      "100 - 1.1264934539794922 - train loss: 0.4095266468251666  val loss: 0.31755260840713323\n",
      "101 - 1.0542078018188477 - train loss: 0.40927810337550335  val loss: 0.31719984980504756\n",
      "102 - 1.053086757659912 - train loss: 0.40903406057415787  val loss: 0.3168556846417282\n",
      "103 - 1.0357012748718262 - train loss: 0.4087932139218697  val loss: 0.3165180583453768\n",
      "104 - 1.0463521480560303 - train loss: 0.40855529847236505  val loss: 0.31619202464327706\n",
      "105 - 1.043316125869751 - train loss: 0.4083204563874843  val loss: 0.3158819962338964\n",
      "106 - 1.0693981647491455 - train loss: 0.40808845099931396  val loss: 0.3155830619360351\n",
      "107 - 1.0506653785705566 - train loss: 0.40785874939230554  val loss: 0.31528289304984797\n",
      "108 - 1.0744376182556152 - train loss: 0.4076310269622155  val loss: 0.31497302766285507\n",
      "109 - 1.136864185333252 - train loss: 0.4074057228139473  val loss: 0.3146567100398038\n",
      "110 - 1.1230194568634033 - train loss: 0.40718374303702276  val loss: 0.31434244522569027\n",
      "111 - 1.1535792350769043 - train loss: 0.4069656005619452  val loss: 0.3140347581740551\n",
      "112 - 1.1075632572174072 - train loss: 0.40675111587810725  val loss: 0.31373358444709865\n",
      "113 - 1.1543197631835938 - train loss: 0.40653970657245064  val loss: 0.3134376137455987\n",
      "114 - 1.0885121822357178 - train loss: 0.40633089229094477  val loss: 0.3131448578007601\n",
      "115 - 1.1847038269042969 - train loss: 0.40612429506734515  val loss: 0.31285151669366673\n",
      "116 - 1.1083076000213623 - train loss: 0.40591944884475023  val loss: 0.3125539339290789\n",
      "117 - 1.1290791034698486 - train loss: 0.4057160325329166  val loss: 0.312250533540334\n",
      "118 - 1.0573513507843018 - train loss: 0.40551370909468043  val loss: 0.3119402111733107\n",
      "119 - 1.1628515720367432 - train loss: 0.4053120658315668  val loss: 0.3116215990528792\n",
      "120 - 1.1576220989227295 - train loss: 0.40511036874462053  val loss: 0.3112898874761803\n",
      "121 - 1.2635412216186523 - train loss: 0.40490671262268096  val loss: 0.3109591928052276\n",
      "122 - 1.1749489307403564 - train loss: 0.404702999427715  val loss: 0.31086156612958066\n",
      "123 - 1.0761001110076904 - train loss: 0.40453086165820296  val loss: 0.31068507040681953\n",
      "124 - 1.1291611194610596 - train loss: 0.40434983672314145  val loss: 0.31022468013919197\n",
      "125 - 1.1203043460845947 - train loss: 0.40414609385222494  val loss: 0.30995747199599316\n",
      "126 - 1.0821809768676758 - train loss: 0.403927363126601  val loss: 0.3095960770516175\n",
      "127 - 1.0789873600006104 - train loss: 0.40369143147964087  val loss: 0.3092470948615081\n",
      "128 - 1.0685217380523682 - train loss: 0.40346331425251647  val loss: 0.30896007939840536\n",
      "129 - 1.083587408065796 - train loss: 0.40326642527945394  val loss: 0.3087129820186829\n",
      "130 - 0.9988112449645996 - train loss: 0.4030822052753361  val loss: 0.30865997971025305\n",
      "131 - 1.0827202796936035 - train loss: 0.4028596202924905  val loss: 0.30853392845429073\n",
      "132 - 1.088477373123169 - train loss: 0.402657660888125  val loss: 0.30826601305986745\n",
      "133 - 1.0077235698699951 - train loss: 0.4024348786957138  val loss: 0.3076886618049344\n",
      "134 - 1.107740879058838 - train loss: 0.4022122928828666  val loss: 0.3073345327284187\n",
      "135 - 1.1434288024902344 - train loss: 0.4020006887749013  val loss: 0.3071925271415039\n",
      "136 - 1.169189453125 - train loss: 0.40181812976752235  val loss: 0.30701928269399514\n",
      "137 - 1.0924639701843262 - train loss: 0.4015883031452974  val loss: 0.3070392461741793\n",
      "138 - 1.1022365093231201 - train loss: 0.40136230178592214  val loss: 0.30695216584909746\n",
      "139 - 1.092083215713501 - train loss: 0.40116153226208234  val loss: 0.3063730092202364\n",
      "140 - 1.0640380382537842 - train loss: 0.4009395080455234  val loss: 0.3057053747745538\n",
      "141 - 0.9761886596679688 - train loss: 0.40072935250457686  val loss: 0.30559884232806633\n",
      "142 - 0.9644730091094971 - train loss: 0.40045457166375026  val loss: 0.30539616306343087\n",
      "143 - 1.0635769367218018 - train loss: 0.40025765113205125  val loss: 0.30525365651707975\n",
      "144 - 1.070732831954956 - train loss: 0.39998837720696956  val loss: 0.30468355517854895\n",
      "145 - 0.9748377799987793 - train loss: 0.3996633308711592  val loss: 0.304233498825272\n",
      "146 - 0.9816944599151611 - train loss: 0.3994096169778156  val loss: 0.3036400171119597\n",
      "147 - 1.0633926391601562 - train loss: 0.3991395157221664  val loss: 0.3032998805891231\n",
      "148 - 1.0014753341674805 - train loss: 0.3988758137230003  val loss: 0.3029078795001484\n",
      "149 - 1.0285720825195312 - train loss: 0.3986178657969663  val loss: 0.3026914129898484\n",
      "150 - 1.0644450187683105 - train loss: 0.39842904438879295  val loss: 0.30243134626525114\n",
      "151 - 1.0797061920166016 - train loss: 0.3982069076484007  val loss: 0.3019835191630441\n",
      "152 - 1.0802688598632812 - train loss: 0.3977738974760876  val loss: 0.3014444777470819\n",
      "153 - 1.1037046909332275 - train loss: 0.3975632023778241  val loss: 0.3013897385591498\n",
      "154 - 1.0062131881713867 - train loss: 0.3972865184598624  val loss: 0.30089459587195583\n",
      "155 - 0.9835035800933838 - train loss: 0.3969725051659528  val loss: 0.30047116318594785\n",
      "156 - 0.9804751873016357 - train loss: 0.3966804156200197  val loss: 0.3007250589236699\n",
      "157 - 0.9932384490966797 - train loss: 0.39637741089060446  val loss: 0.30005522929183154\n",
      "158 - 0.9914798736572266 - train loss: 0.3961284184493515  val loss: 0.2997576406220445\n",
      "159 - 0.9989614486694336 - train loss: 0.39578811588296703  val loss: 0.2991998138336035\n",
      "160 - 1.017472267150879 - train loss: 0.39547210983647874  val loss: 0.2988979610138583\n",
      "161 - 1.011178731918335 - train loss: 0.39516161275149214  val loss: 0.29863181153240687\n",
      "162 - 1.0969486236572266 - train loss: 0.39478160527186557  val loss: 0.29881872095239276\n",
      "163 - 0.9764626026153564 - train loss: 0.3945156889404372  val loss: 0.29830562321540827\n",
      "164 - 0.9859087467193604 - train loss: 0.3941661762738365  val loss: 0.2980463219970301\n",
      "165 - 0.9825687408447266 - train loss: 0.39390295544035325  val loss: 0.2974284374472368\n",
      "166 - 0.9972305297851562 - train loss: 0.3938433982582769  val loss: 0.2980881258922087\n",
      "167 - 1.0445013046264648 - train loss: 0.3932604566602924  val loss: 0.2970112482377502\n",
      "168 - 1.2113282680511475 - train loss: 0.39291651228424535  val loss: 0.2963983878457489\n",
      "169 - 1.0578501224517822 - train loss: 0.3925869579646116  val loss: 0.2962655693466601\n",
      "170 - 1.012399435043335 - train loss: 0.3922546518591078  val loss: 0.2961622173642064\n",
      "171 - 1.0060560703277588 - train loss: 0.39193027892747123  val loss: 0.29595801889753115\n",
      "172 - 1.0069215297698975 - train loss: 0.39163176562071483  val loss: 0.2956044434743759\n",
      "173 - 1.1085662841796875 - train loss: 0.39129910669614393  val loss: 0.294935561094822\n",
      "174 - 1.145695686340332 - train loss: 0.39095836301126646  val loss: 0.2944537945025201\n",
      "175 - 1.3557002544403076 - train loss: 0.39062488215999464  val loss: 0.2940510289236944\n",
      "176 - 1.2632091045379639 - train loss: 0.3903045112780538  val loss: 0.29380234328078125\n",
      "177 - 1.0537364482879639 - train loss: 0.3899903845169256  val loss: 0.29383578924922743\n",
      "178 - 1.0057463645935059 - train loss: 0.3896807617321317  val loss: 0.29327826790900513\n",
      "179 - 1.1191246509552002 - train loss: 0.38957998965691476  val loss: 0.29417584728510987\n",
      "180 - 1.0673871040344238 - train loss: 0.3895588161736158  val loss: 0.29334800678131345\n",
      "181 - 1.0152759552001953 - train loss: 0.38883011950859153  val loss: 0.29268734297295523\n",
      "182 - 1.070986032485962 - train loss: 0.38841063574127793  val loss: 0.29291036591230596\n",
      "183 - 1.0160820484161377 - train loss: 0.388190819044866  val loss: 0.29181458506794583\n",
      "184 - 1.0156605243682861 - train loss: 0.38789086335517614  val loss: 0.2914571236783209\n",
      "185 - 1.142087459564209 - train loss: 0.3876204704284056  val loss: 0.29098865918170375\n",
      "186 - 1.06288480758667 - train loss: 0.3873770416251781  val loss: 0.2912794032957978\n",
      "187 - 0.9993691444396973 - train loss: 0.3870849278057569  val loss: 0.2899336372758006\n",
      "188 - 1.0060675144195557 - train loss: 0.3866633853914416  val loss: 0.2895844591356773\n",
      "189 - 1.065687656402588 - train loss: 0.3863198804959993  val loss: 0.28950457772301447\n",
      "190 - 1.127077579498291 - train loss: 0.3859411177300787  val loss: 0.2889269596160195\n",
      "191 - 1.066164255142212 - train loss: 0.3856919332318508  val loss: 0.2883958379766177\n",
      "192 - 1.1071650981903076 - train loss: 0.3854097036969745  val loss: 0.2884123584966861\n",
      "193 - 1.2403020858764648 - train loss: 0.38515127576420927  val loss: 0.28789426579145244\n",
      "194 - 1.2086942195892334 - train loss: 0.38484909252969246  val loss: 0.28790938485782225\n",
      "195 - 1.0697746276855469 - train loss: 0.3845226944469463  val loss: 0.2868414764819605\n",
      "196 - 1.0409772396087646 - train loss: 0.3841798930889924  val loss: 0.28678219446114134\n",
      "197 - 1.1026599407196045 - train loss: 0.3839106181647111  val loss: 0.2868722618920831\n",
      "198 - 1.0718073844909668 - train loss: 0.38384560338722384  val loss: 0.28791614635693996\n",
      "199 - 1.0866687297821045 - train loss: 0.3834875754023973  val loss: 0.28725558626311987\n",
      "200 - 1.1959547996520996 - train loss: 0.38338242890099455  val loss: 0.28562116738444626\n",
      "201 - 1.1746397018432617 - train loss: 0.38294908131773125  val loss: 0.28635084121436866\n",
      "202 - 1.1050794124603271 - train loss: 0.38271619265568707  val loss: 0.2849879538062141\n",
      "203 - 1.0848708152770996 - train loss: 0.3826059307866033  val loss: 0.2855388105048665\n",
      "204 - 1.2257192134857178 - train loss: 0.3822851448563405  val loss: 0.2841770647958453\n",
      "205 - 1.1335954666137695 - train loss: 0.3820653059969788  val loss: 0.28438292979646557\n",
      "206 - 1.1659884452819824 - train loss: 0.38193203775903894  val loss: 0.28410822527283536\n",
      "207 - 1.1071701049804688 - train loss: 0.38168349689253217  val loss: 0.2836080367080765\n",
      "208 - 1.1329338550567627 - train loss: 0.381582277945997  val loss: 0.28443813673220575\n",
      "209 - 1.114128828048706 - train loss: 0.3813816030773236  val loss: 0.2827669788435982\n",
      "210 - 1.1206209659576416 - train loss: 0.38136262457603726  val loss: 0.2848731206018723\n",
      "211 - 1.15887451171875 - train loss: 0.3810008095691668  val loss: 0.28812573339718467\n",
      "212 - 1.0696830749511719 - train loss: 0.3812851872611518  val loss: 0.28790164472801344\n",
      "213 - 1.0205423831939697 - train loss: 0.38066283990347555  val loss: 0.28243683808698106\n",
      "214 - 1.1569302082061768 - train loss: 0.38054960212783745  val loss: 0.28291758977818293\n",
      "215 - 1.085387945175171 - train loss: 0.3804744587997815  val loss: 0.28301241143688477\n",
      "216 - 1.0920259952545166 - train loss: 0.38016733204686287  val loss: 0.2814693991887987\n",
      "217 - 1.0683624744415283 - train loss: 0.3802003497702045  val loss: 0.2813559859891289\n",
      "218 - 1.106837511062622 - train loss: 0.3799622061873251  val loss: 0.28189695915744567\n",
      "219 - 1.1421024799346924 - train loss: 0.37988091801736645  val loss: 0.28257540995408625\n",
      "220 - 1.0983679294586182 - train loss: 0.3798421102438353  val loss: 0.2951325405117861\n",
      "221 - 1.0526444911956787 - train loss: 0.3811117923090167  val loss: 0.28300623077346565\n",
      "222 - 1.101731300354004 - train loss: 0.37963371808505225  val loss: 0.28082602059309997\n",
      "223 - 1.0483200550079346 - train loss: 0.37947809109988284  val loss: 0.2821765853306995\n",
      "224 - 0.9984390735626221 - train loss: 0.3792831554253912  val loss: 0.2813218075623429\n",
      "225 - 1.004408836364746 - train loss: 0.3792758189683809  val loss: 0.281323000449677\n",
      "226 - 1.0547597408294678 - train loss: 0.37939755185703816  val loss: 0.28089208643151714\n",
      "227 - 1.0089337825775146 - train loss: 0.37910814209926846  val loss: 0.28194261820229527\n",
      "228 - 0.9802637100219727 - train loss: 0.37901301624212164  val loss: 0.28350996948231094\n",
      "229 - 1.1258797645568848 - train loss: 0.37946401260217644  val loss: 0.28033130883065227\n",
      "230 - 1.2384655475616455 - train loss: 0.3791049619850176  val loss: 0.2806336951690245\n",
      "231 - 1.1167471408843994 - train loss: 0.37897233499254007  val loss: 0.2800461119271119\n",
      "232 - 1.0867726802825928 - train loss: 0.37865086699875655  val loss: 0.28154395786907194\n",
      "233 - 1.1862430572509766 - train loss: 0.3786458443883978  val loss: 0.28026848313639163\n",
      "234 - 1.118220567703247 - train loss: 0.37850842507376864  val loss: 0.28026761353184765\n",
      "235 - 1.0774271488189697 - train loss: 0.37847438878928014  val loss: 0.2813547399857051\n",
      "236 - 1.0980727672576904 - train loss: 0.3784111866968982  val loss: 0.2801430351100862\n",
      "237 - 1.0607943534851074 - train loss: 0.37859458405131463  val loss: 0.2819687895883632\n",
      "238 - 1.0766258239746094 - train loss: 0.3781224661988904  val loss: 0.28142178613196156\n",
      "239 - 1.0775084495544434 - train loss: 0.3787995946907342  val loss: 0.2850914838081635\n",
      "240 - 1.0363194942474365 - train loss: 0.37837393337710973  val loss: 0.28637914819474036\n",
      "241 - 1.0905487537384033 - train loss: 0.3783831332385537  val loss: 0.28070677036373454\n",
      "242 - 1.2278447151184082 - train loss: 0.37814820541177796  val loss: 0.28038555085479927\n",
      "early stopping......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_290528/1092730607.py:212: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(MODEL_NAME + \".pt\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def loss_function(recon_x, x, mu, log_var):\n",
    "    x_data = x[:, window_size-1, :]\n",
    "    reconstruction_loss = nn.functional.mse_loss(recon_x, x_data.view(-1, input_dim), reduction='sum')\n",
    "\n",
    "    kl_divergence = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return reconstruction_loss + 0.002 * kl_divergence\n",
    "\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        attention_weights = self.attention(inputs)\n",
    "        weighted_input = inputs * attention_weights\n",
    "        return weighted_input, attention_weights\n",
    "\n",
    "class MSEFeedbackRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(MSEFeedbackRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.unsqueeze(1)\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = self.fc(out)  # Apply the fully connected layer to each time step\n",
    "        return out\n",
    "\n",
    "# CompositeModel: AE-FAR or VAE-FAR according to the autoencoder type\n",
    "class CompositeModel(nn.Module):\n",
    "    def __init__(self, autoencoder, rnn, attention_dim=64):\n",
    "        super(CompositeModel, self).__init__()\n",
    "        self.autoencoder = autoencoder\n",
    "        self.rnn = rnn\n",
    "        self.attention = AttentionLayer(input_dim= input_dim+1, hidden_dim=attention_dim)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        reconstructed, mean, log = self.autoencoder(x) # For VAE\n",
    "        # reconstructed = self.autoencoder(x) #For AE\n",
    "        mse_error = ((y - reconstructed) ** 2).mean(dim=1, keepdim=True)\n",
    "\n",
    "        combined_input = torch.cat((reconstructed, mse_error), dim=1)\n",
    "        combined_input, attention_weights = self.attention(combined_input)\n",
    "\n",
    "        rnn_output = self.rnn(combined_input)\n",
    "        rnn_output = rnn_output.squeeze(1)\n",
    "        \n",
    "        adjusted_reconstructed = reconstructed + rnn_output\n",
    "        return adjusted_reconstructed, mse_error, rnn_output\n",
    "\n",
    "\n",
    "\n",
    "if flag_train_composite:\n",
    "    autoencoder = torch.load(MODEL_PATH[:-4] + \".pt\", map_location=device)\n",
    "    for param in autoencoder.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    rnn_hidden_size = input_dim//2\n",
    "    rnn_output_size = input_dim\n",
    "\n",
    "    rnn = MSEFeedbackRNN(input_size= input_dim + 1, hidden_size=rnn_hidden_size, output_size=rnn_output_size, num_layers=8)\n",
    "\n",
    "    # AR-FAR or VAE-FAR according to the selected AutoEncoder\n",
    "    model = CompositeModel(autoencoder, rnn)\n",
    "\n",
    "else:\n",
    "    if flag_AE:\n",
    "        from my_models import CustomModel\n",
    "        model = CustomModel(window_size, input_dim)\n",
    "    else:\n",
    "        # ******* TRAIN only VAh_catE **************************\n",
    "        from my_models import LSTM_VAE\n",
    "        model = LSTM_VAE(window_size, input_size=input_dim, hidden_size=64, latent_size=32)\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "X_train = torch.tensor(train_data, dtype=torch.float32).to(device)\n",
    "X_valid = torch.tensor(val_data, dtype=torch.float32).to(device)\n",
    "\n",
    "print(X_train.shape)\n",
    "\n",
    "# loss function and optimizer\n",
    "loss_fn = nn.MSELoss()  # mean square error\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "\n",
    "# training parameters\n",
    "batch_size = 128  # size of each batch\n",
    "\n",
    "# Hold the best model\n",
    "best_loss_val = np.inf   # init to infinity\n",
    "best_weights = None\n",
    "history_train, history_val = [], []\n",
    "\n",
    "stop_counter, early_stop = 0, 10\n",
    "# training loop\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    loss_train = []\n",
    "    start_time = time.time()\n",
    "    with tqdm.tqdm(iterate_batches(X_train, window_size, batch_size), unit=\"batch\", mininterval=0, disable=True) as bar:\n",
    "        bar.set_description(f\"Epoch {epoch}\")\n",
    "        for X_batch, y_batch in bar:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if flag_train_composite:\n",
    "                reconstructed, mse_error, rnn_output = model(X_batch, y_batch)\n",
    "                loss = loss_fn(reconstructed, y_batch)\n",
    "            elif flag_AE:\n",
    "                 # ***** Simple AE ********\n",
    "                y_pred = model(X_batch)\n",
    "                loss = loss_fn(y_pred, y_batch)\n",
    "            else:\n",
    "            # ***** VAE ********\n",
    "                y_pred, mean, log_var = model(X_batch)\n",
    "                loss = loss_function(y_pred, X_batch, mean, log_var)\n",
    "\n",
    "           \n",
    "\n",
    "            loss_train.append(loss.item())\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            # update weights\n",
    "            optimizer.step()\n",
    "            # print progress\n",
    "            bar.set_postfix(mse=float(loss))\n",
    "\n",
    "    # evaluate accuracy at end of each epoch\n",
    "    model.eval()\n",
    "    loss_valid = []\n",
    "    for X_batch, y_batch in iterate_batches(X_valid, window_size, batch_size):\n",
    "         \n",
    "        if flag_train_composite:\n",
    "            reconstructed, mse_error, rnn_output = model(X_batch, y_batch)\n",
    "            loss = loss_fn(reconstructed, y_batch)\n",
    "        elif flag_AE:\n",
    "                # ***** Simple AE ********\n",
    "            y_pred = model(X_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "        else:\n",
    "        # ***** VAE ********\n",
    "            y_pred, mean, log_var = model(X_batch)\n",
    "            loss = loss_function(y_pred, X_batch, mean, log_var)\n",
    "\n",
    "        loss_valid.append(loss.item())\n",
    "\n",
    "    loss_valid = np.mean(loss_valid)\n",
    "    loss_train = np.mean(loss_train)\n",
    "    history_train.append(loss_train)\n",
    "    history_val.append(loss_valid)\n",
    "    print(\"{0} - {1} - train loss: {2}  val loss: {3}\".format(epoch, time.time()-start_time, loss_train, loss_valid))\n",
    "    if loss_valid < best_loss_val:\n",
    "        best_loss_val = loss_valid\n",
    "        torch.save(model, MODEL_PATH + \".pt\")\n",
    "        stop_counter = 0\n",
    "    else:\n",
    "        stop_counter +=1\n",
    "\n",
    "    if stop_counter > early_stop:\n",
    "        print(\"early stopping......\")\n",
    "        break\n",
    "\n",
    "# restore model with best accuracy\n",
    "model = torch.load(MODEL_PATH + \".pt\")\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open(MODEL_PATH + \".history\", 'wb') as f:\n",
    "    pickle.dump([history_train, history_val], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f98edc",
   "metadata": {
    "id": "94f98edc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test  73729\n",
      "predictions  73679\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "X_test = torch.tensor(test_data, dtype=torch.float32).to(\"cuda\")\n",
    "\n",
    "predictions = None\n",
    "for batch, y_batch in iterate_batches(X_test, window_size, batch_size):\n",
    "    \n",
    "    if flag_train_composite:\n",
    "        y_pred, mse_error, rnn_output = model(batch, y_batch)\n",
    "    elif flag_AE:\n",
    "        y_pred = model(batch)\n",
    "    else:\n",
    "        y_pred, mean, log_var = model(batch)    \n",
    "    \n",
    "    y_pred = y_pred.cpu().detach().numpy()\n",
    "\n",
    "    if predictions is None:\n",
    "        predictions = y_pred\n",
    "    else:\n",
    "        predictions = np.concatenate((predictions, y_pred), axis=0)\n",
    "\n",
    "print(\"X_test \", len(X_test))\n",
    "print(\"predictions \", len(predictions))\n",
    "\n",
    "test_data_tmp = test_data[window_size:]\n",
    "true_labels = test_labels[window_size:]\n",
    "\n",
    "print(\"test_data_tmp \", len(test_data_tmp))\n",
    "print(\"true_labels \", len(true_labels))\n",
    "\n",
    "mse = np.mean(np.power(test_data_tmp - predictions, 2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832e685a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"SWAT\" in MODEL_PATH:\n",
    "    th_factor = 5.0\n",
    "elif \"SMD\" in MODEL_PATH:\n",
    "    th_factor = 5.0\n",
    "elif \"MSL\" in MODEL_PATH:\n",
    "    th_factor = 6.5\n",
    "elif \"PSM\" in MODEL_PATH:\n",
    "    th_factor = 4.0\n",
    "elif \"SMAP\" in MODEL_PATH:\n",
    "    th_factor = 6.5\n",
    "\n",
    "print(\"dataset: \", dataset)\n",
    "print(\"model: \", MODEL_PATH)\n",
    "print(\"threshold: \", th_factor)\n",
    "\n",
    "\n",
    "pred_y, dynamic_threshold = sliding_window_anomaly_detection(mse, window_size, threshold_factor=th_factor)\n",
    "gt, pred_adjusted = apply_adjustment(true_labels, pred_y)\n",
    "print(\"adjusted with sliding: \", get_precision_recall_f1(gt, pred_adjusted))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
