{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2dc41424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of train:  708405\n",
      "# of test:  708420\n",
      "number of 1s in test:  29444\n",
      "number of 0s in test:  678976\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "window_size = 50  # size of the window\n",
    "\n",
    "\n",
    "data_path = \"../Anomaly/DCDetector_dataset/SMD/\"\n",
    "input_dim = 38\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data = np.load(data_path + \"/SMD_train.npy\")[:,:]\n",
    "scaler.fit(data)\n",
    "data = scaler.transform(data)\n",
    "test_data = np.load(data_path + \"/SMD_test.npy\")[:,:]\n",
    "test = scaler.transform(test_data)\n",
    "train_data = data\n",
    "data_len = len(train_data)\n",
    "val_data = train_data[(int)(data_len * 0.8):]\n",
    "test_labels = np.load(data_path + \"/SMD_test_label.npy\")[:]\n",
    "\n",
    "\n",
    "# data_path = \"../Anomaly/DCDetector_dataset/SWAT/\"\n",
    "# input_dim = 51\n",
    "# train_data = pd.read_csv( data_path + 'swat_train2.csv')\n",
    "# test_data = pd.read_csv(data_path + 'swat2.csv')\n",
    "\n",
    "# test_labels = test_data.values[:, -1]\n",
    "# train_data = train_data.values[:, :-1]\n",
    "# test_data = test_data.values[:, :-1]\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(train_data)\n",
    "\n",
    "# train_data = scaler.transform(train_data)\n",
    "# test_data = scaler.transform(test_data)\n",
    "# data_len = len(train_data)\n",
    "# val_data = train_data[(int)(data_len * 0.8):]\n",
    "\n",
    "\n",
    "# data_path = \"../Anomaly/DCDetector_dataset/MSL/\"\n",
    "# input_dim = 55\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# train_data = np.load(data_path + \"/MSL_train.npy\")\n",
    "# scaler.fit(train_data)\n",
    "# train_data = scaler.transform(train_data)\n",
    "# test_data = np.load(data_path + \"/MSL_test.npy\")\n",
    "# test_data = scaler.transform(test_data)\n",
    "# test_labels = np.load(data_path + \"/MSL_test_label.npy\")\n",
    "# # val_data = test_data\n",
    "# data_len = len(train_data)\n",
    "# val_data = train_data[(int)(data_len * 0.8):]\n",
    "\n",
    "\n",
    "print(\"# of train: \", len(train_data))\n",
    "print(\"# of test: \", len(test_data))\n",
    "\n",
    "\n",
    "df_test_0 = test_labels[test_labels == 0]\n",
    "df_test_1 = test_labels[test_labels == 1]\n",
    "\n",
    "print(\"number of 1s in test: \", len(df_test_1))\n",
    "print(\"number of 0s in test: \", len(df_test_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c17acd64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_323023/1114789134.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(MODEL_NAME + \".pt\", map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test  708420\n",
      "predictions  708370\n"
     ]
    }
   ],
   "source": [
    "window_size = 50  # size of the window\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        attention_weights = self.attention(inputs)\n",
    "        weighted_input = inputs * attention_weights\n",
    "        return weighted_input, attention_weights\n",
    "\n",
    "class MSEFeedbackRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(MSEFeedbackRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.unsqueeze(1)\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = self.fc(out)  # Apply the fully connected layer to each time step\n",
    "        return out\n",
    "\n",
    "\n",
    "class CompositeModel(nn.Module):\n",
    "    def __init__(self, autoencoder, rnn, attention_dim=64):\n",
    "        super(CompositeModel, self).__init__()\n",
    "        self.autoencoder = autoencoder\n",
    "        self.rnn = rnn\n",
    "        self.attention = AttentionLayer(input_dim= input_dim+1, hidden_dim=attention_dim)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # reconstructed, mean, log = self.autoencoder(x) # For VAE\n",
    "        reconstructed = self.autoencoder(x) #For AE\n",
    "        mse_error = ((y - reconstructed) ** 2).mean(dim=1, keepdim=True)\n",
    "\n",
    "        combined_input = torch.cat((reconstructed, mse_error), dim=1)\n",
    "        combined_input, attention_weights = self.attention(combined_input)\n",
    "\n",
    "        rnn_output = self.rnn(combined_input)\n",
    "        rnn_output = rnn_output.squeeze(1)\n",
    "        \n",
    "        adjusted_reconstructed = reconstructed + rnn_output\n",
    "        return adjusted_reconstructed, mse_error, rnn_output\n",
    "\n",
    "\n",
    "if \"SWAT\" in data_path:\n",
    "    MODEL_NAME = \"models/SWAT/SWAT-AE-FAR\"\n",
    "if \"MSL\" in data_path:\n",
    "    MODEL_NAME = \"models/MSL/MSL-AE-FAR\"\n",
    "if \"SMD\" in data_path:\n",
    "    MODEL_NAME = \"models/SMD/SMD-AE-FAR\"\n",
    "\n",
    "model = torch.load(MODEL_NAME + \".pt\", map_location=device)\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "batch_size = 128  # size of each batch\n",
    "\n",
    "\n",
    "# to predict a single target value, not the entire window\n",
    "def iterate_batches(data, window_size, batch_size, start_idx = 0):\n",
    "    for start in range(start_idx, len(data) - window_size, batch_size):\n",
    "        end = min(start + batch_size, len(data) - window_size)\n",
    "        batch_data = [data[i:i + window_size] for i in range(start, end)]\n",
    "        batch_targets = [data[i + window_size] for i in range(start, end)]\n",
    "        yield torch.stack(batch_data), torch.stack(batch_targets)\n",
    "\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "X_test = torch.tensor(test_data, dtype=torch.float32).to(device)\n",
    "\n",
    "predictions = None\n",
    "for batch, y_batch in iterate_batches(X_test, window_size, batch_size):\n",
    "    # y_pred = model(batch)\n",
    "    y_pred, mean, log_var = model(batch, y_batch)\n",
    "    \n",
    "    y_pred = y_pred.cpu().detach().numpy()\n",
    "    \n",
    "    if predictions is None:\n",
    "        predictions = y_pred\n",
    "    else:\n",
    "        predictions = np.concatenate((predictions, y_pred), axis=0)\n",
    "\n",
    "print(\"X_test \", len(X_test))\n",
    "print(\"predictions \", len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "94f1c810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_adjustment(gt_, pred_):\n",
    "    gt = gt_.copy()\n",
    "    pred = pred_.copy()\n",
    "    anomaly_state = False\n",
    "    for i in range(len(gt)):\n",
    "        if gt[i] == 1 and pred[i] == 1 and not anomaly_state:\n",
    "            anomaly_state = True\n",
    "            for j in range(i, 0, -1):\n",
    "                if gt[j] == 0:\n",
    "                    break\n",
    "                else:\n",
    "                    if pred[j] == 0:\n",
    "                        pred[j] = 1\n",
    "            for j in range(i, len(gt)):\n",
    "                if gt[j] == 0:\n",
    "                    break\n",
    "                else:\n",
    "                    if pred[j] == 0:\n",
    "                        pred[j] = 1\n",
    "        elif gt[i] == 0:\n",
    "            anomaly_state = False\n",
    "        if anomaly_state:\n",
    "            pred[i] = 1\n",
    "    return gt, pred\n",
    "# end function\n",
    "\n",
    "def sliding_window_anomaly_detection(mse_list, window_size, threshold_factor=3):\n",
    "    mse_series = pd.Series(mse_list)\n",
    "    \n",
    "    # Calculate moving average and moving standard deviation\n",
    "    moving_avg = mse_series.rolling(window=window_size, min_periods=1).mean()\n",
    "    moving_std = mse_series.rolling(window=window_size, min_periods=1).std()\n",
    "    \n",
    "    # Calculate dynamic threshold\n",
    "    dynamic_threshold = moving_avg + (threshold_factor * moving_std)\n",
    "    \n",
    "    # Identify anomalies\n",
    "    anomalies = (mse_series > dynamic_threshold).astype(int)\n",
    "    10\n",
    "    # Convert to list for output\n",
    "    anomalies_list = anomalies.tolist()\n",
    "    \n",
    "    return anomalies_list, dynamic_threshold.tolist()\n",
    "\n",
    "def get_precision_recall_f1(true_labels, pred_y):\n",
    "    precision, recall, f1_score, _ = precision_recall_fscore_support(true_labels, pred_y, average='binary')\n",
    "    return round(precision, 4), round(recall, 4), round(f1_score, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ca9cc64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold:  0.12\n",
      "adjusted:  (0.6272, 0.5201, 0.5686)\n",
      "adjusted with sliding:  (0.9201, 0.882, 0.9006)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if \"SWAT\" in MODEL_NAME:\n",
    "    threshold_fixed = 7 #SWAT\n",
    "    th_factor = 5\n",
    "\n",
    "elif \"SMD\" in MODEL_NAME:\n",
    "    threshold_fixed = 0.12\n",
    "    th_factor = 4.1\n",
    "\n",
    "elif \"MSL\" in MODEL_NAME:\n",
    "    threshold_fixed = 1\n",
    "    th_factor = 6.5\n",
    "\n",
    "\n",
    "test_data_tmp = test_data[window_size:]\n",
    "true_labels = test_labels[window_size:]\n",
    "\n",
    "mse = np.mean(np.power(test_data_tmp - predictions, 2), axis=1)\n",
    "\n",
    "# threshold_fixed = np.percentile(mse, 99)\n",
    "print(\"threshold: \", threshold_fixed)\n",
    "\n",
    "pred_y = [1 if e > threshold_fixed else 0 for e in mse]\n",
    "\n",
    "gt, pred_adjusted = apply_adjustment(true_labels, pred_y)\n",
    "print(\"adjusted: \", get_precision_recall_f1(gt, pred_adjusted))\n",
    "\n",
    "pred_y, dynamic_threshold = sliding_window_anomaly_detection(mse, window_size, threshold_factor=th_factor)\n",
    "gt, pred_adjusted = apply_adjustment(true_labels, pred_y)\n",
    "print(\"adjusted with sliding: \", get_precision_recall_f1(gt, pred_adjusted))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
